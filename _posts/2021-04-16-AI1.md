---
title:  "[2~6주차] 인공지능"
excerpt: "Linear regression"

categories:
  - 
tags:
  - 
last_modified_at: 2021-04-17 #T08:06:00-05:00
---

---
# Formulation
### Variables
- input : feature or sample
- output : target or label
- Training example : The pair (tuple)
- The dataset : list of training examples
- functuin : model

### Custom
- column vector

---
### Linear function
- h(x) = wx + b

### Cost Function (least-squares)
- (i: 1, ..., m)
- J(b,w) = Σ(h(x^(i)) - y^(i))^2 / m &nbsp; = &nbsp; Σ((wx^(i) + b) - y^(i))^2 / m
- Cost Function의 return이 최소값이 되는 b, w를 찾는다.
- return 값이 제일 작은 것이 좋은 예측기

---
# Supervised learning
- 정답을 알려주며 학습시키는 것
### Linear regression

---
# "loss function" or "cost function"
- model이 좋은 model인지 측정하기 위한 함수

### Mean squared error (MSE)
### Minimum Mean squared error (MMSE)
- J(θ) = Σ│h(x^(i)) - y^(i)│^2 / m &nbsp; or &nbsp; Σ│h(x^(i)) - y^(i)│^2 / 2
- min : J(θ)의 최소값 # Y축
- arg mim : J(θ)를 최소화하는 θ값 # X축
- 1/m 대신 1/2 쓰는 이유 : J(θ) 미분했을 때 2를 약분하기 위해서

- θ* = Σ y^(i) x^(i) / Σ (x^(i))^2

---
# Gradient Descent
- 미분 값이 0이 되는 포인트(critical point)를 찾는 알고리즘
- 최소 점을 찾는 것이 아닌 최소 점을 찾는 것을 유도한다.

### learning rate
- 다음θ = 전θ - α(전θ 기울기)
- α = learning rate
- 학습을 하는 parameter가 아니고 학습을 위해 정하는 parameter이다. => Hyper parameter

### epoch
- Update time for one full set of training data

### including a Bias
- hθ(x) = θ1x + θ0 (θ0 = bias)
- θ1x, θ0 각각 편미분